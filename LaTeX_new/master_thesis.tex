% This is the new version!

\documentclass[12pt, a4paper]{article}
\usepackage[left=3cm, right = 2cm, vmargin=2cm]{geometry}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[english]{babel}
\usepackage{setspace}
\usepackage{xcolor}
\usepackage{multicol}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{titlesec}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage[font=small,skip=2pt]{caption}
\usepackage[
backend=biber,
style=authoryear,
]{biblatex}
\usepackage{amsthm}% http://ctan.org/pkg/amsthm

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newtheoremstyle{MAstyle}
{\topsep} % Space above
{\topsep} % Space below
{} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{\newline} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{MAstyle} \newtheorem{assumption}{Assumption}[section]
\theoremstyle{MAstyle} \newtheorem{definition}{Definition}[section]
\theoremstyle{MAstyle} \newtheorem{theorem}{Theorem}[section]

\titleformat*{\section}{\Large\bfseries}
\titleformat*{\subsection}{\large\bfseries}

\addbibresource{thesis_bibliography.bib}
\onehalfspacing
\parindent=0pt

\begin{document}
	
	\title{{\huge Bugni and Horowitz (2021) Permutation Tests\\ for the Equality of Distributions of\\ Functional Data}}
	\date{}
	\maketitle
	\thispagestyle{empty}
	\vspace{1.5 cm}
	\begin{center}
		
		\Large
		Master's Thesis presented to the\\
		Department of Economics at the\\
		Rheinische Friedrich-Wilhelms-Universit√§t Bonn
		\vspace{1.5cm}

		\large
		In Partial Fulfillment of the Requirements for the Degree of\\
		Master of Science (M.Sc.)
		
		\vspace{3cm}
		
		Supervisor: Prof. Dr. Dominik Liebl
		
		\vspace{3cm}
		
		Submitted in June 2022 by: \\
		Jakob R. Juergens\\
		Matriculation Number: 2996491
	\end{center}
	
	\newpage
	
	\thispagestyle{empty}
	\tableofcontents
	\thispagestyle{empty}
	
	\newpage
	\pagenumbering{arabic}
	
	\section{Introduction}
		In modern economics, it is becoming more and more common to use data measured at a very high frequency. As the frequency of observing a variable increases, it often becomes natural to view the data not as a sequence of distinct observations but as a smooth curve describing the variable.
		This idea, to think of observations as measurements of a continuous process, is the motivating thought behind functional data analysis. Functional data analysis is a branch of statistics that has its beginnings in the 1940s and 1950s in the works of Ulf Grenander and Kari Karhunen. It gained traction during the following decades and focused more on possible applications during the 1990s. Functional data analysis is still a relatively exotic field in economics, but it is beginning to gain traction.
		A typical question in economics is whether observations from two or more data sets, e.g., data generated by treatment and control groups, are systematically different across groups. In statistical terms, this can be formulated as whether the same stochastic process generated observations in both data sets.				
		This question also occurs in functional data analysis, where each observation in a data set is itself a smooth curve. The main focus of this thesis is a test developed in \cite{bugni_permutation_2021} trying to answer this question.\\
		
		This thesis aims to present the aforementioned test while giving the necessary theoretical context. Therefore, Section \ref{FDA} introduces the necessary concepts from functional data analysis. 
		Section \ref{CvM_Tests} explores Cram\'{e}r-von Mises tests in a scalar setting and Section \ref{Permutation_Tests} introduces the necessary background in Permutation Testing.
		After explaining these concepts, section \ref{Bugni_Horowitz_2021} focuses on the test developed in \cite{bugni_permutation_2021} for the case of a two-sample test.
		The main contribution of this thesis is a variant of the original test with the goal to identify specific violations of the null hypothesis relating to the persistence of the data generating processes. This variant of the original test is presented in Section \ref{variant}.
		Section \ref{Simulation_Study} presents a set of simulations exploring the properties of the original test and shows a heuristic simulation to motivate the proposed extension. 
		Section \ref{Application} applies the test from \cite{bugni_permutation_2021} to half-hourly electricity demand data from Adelaide.
		Finally, Section \ref{Outlook} gives an Outlook on possible further extensions of the underlying idea, addresses some problems and shortcomings of the presented results and outlines potential simulations that could be performed to better understand the properties of the procedures described in this thesis.\\
		
		Additionally, Appendix \ref{Intuition} gives a very informal description of the test scenario and the underlying idea of the test. It shall serve as a primer for readers without prior experience in functional data analysis and permutation testing. 
	
	\section{Functional Data Analysis}\label{FDA}
		The overarching concept of functional data analysis is to incorporate observations that are functional in nature. In this context, a functional observation can often be understood as a smooth curve. A classical example of this is shown in Figure \ref{growth_curves}. It presents data provided in the R package \textit{fda}\footcite{fda} and shows growth curves of 93 humans up to the age of 18.
		\begin{figure}[H]
			\makebox[\textwidth][c]{
			\includegraphics[width = 1.1\textwidth]{../Graphics/growth\_curves.PDF}
		}
			\caption{Human Growth Curves up to the Age of 18}
			\label{growth_curves}
		\end{figure}
		Even though the measurements were taken at discrete ages, it is clear that each human has a height at every point in time. The data points are only measurements of this continuous curve. The higher the measurement frequency, the closer we get to data that resembles the curve itself.
		In many cases functional data analysis restricts its scope to subsets of the functions $f:\mathbb{R} \rightarrow \mathbb{R}$.
		As these are inherently infinite-dimensional, it is necessary to introduce additional theory to appropriately deal with their unique properties. Sections \ref{Square_Integrable_Functions} and \ref{bases_L2} closely follow \cite{hsing_theoretical_2015} who provide a detailed introduction into the theory of functional data analysis.
		
		\begin{itemize}
			\item \cite{ramsay_functional_2005}
			\item \cite{kokoszka_introduction_2021}
		\end{itemize}
	
		\subsection{Hilbert Space of Square Integrable Functions}\label{Square_Integrable_Functions}
			\begin{definition}[Inner Product]
				A function $\langle \cdot , \cdot \rangle : \mathbb{V}^2 \rightarrow \mathbb{F}$ on a vector space $\mathbb{V}$ over a field $\mathbb{F}$ is called an inner product if the following four conditions hold for all $v, v_1, v_2 \in \mathbb{V}$ and $a_1, a_2 \in \mathbb{F}$.
				\begin{multicols}{2}
					\begin{enumerate}
						\item $\langle v,v \rangle \geq 0$
						\item $\langle v,v \rangle = 0$ if $v = 0$
						\item $\langle a_1 v_1 + a_2 v_2, v \rangle = a_1 \langle v_1, v \rangle + a_2 \langle v_2, v \rangle$
						\item $\langle v_1, v_2 \rangle = \overline{\langle v_2, v_1 \rangle}$
					\end{enumerate}
				\end{multicols}
			\end{definition}
			As this thesis is limited to the case $\mathbb{F} = \mathbb{R}$, property 4 can be restated as $\langle v_1, v_2 \rangle = \langle v_2, v_1 \rangle$, as the complex conjugate of a real number is the number itself. Similar to the case of Euclidean space, we say that two elements $v_1$ and $v_2$ of the inner product space are orthogonal if $\langle v_1, v_2 \rangle = 0$.
			
			\begin{definition}[Inner Product Spaces and Hilbert Spaces]
				A vector space with an associated inner product is called an inner product space.
				An inner product space that is complete with respect to the distance induced by the norm $\| v \| = \sqrt{\langle v, v\rangle}$ is called a Hilbert space.
			\end{definition}
			Hilbert spaces play an important role in functional data analysis and many methods such as for example functional linear regression make extensive use of their properties. In the context of this thesis, the idea of Hilbert spaces is necessary to adequately describe objects such as random functions or probability measures on Hilbert spaces that are used extensively in \cite{bugni_permutation_2021}.\\
			
			Analogously to the case of a vector space, it is useful to express elements of a Hilbert space as linear combinations of a set of elements. However, as elements of Hilbert spaces can be potentially infinite dimensional, the classical idea of a finite basis that can be used to express every element has to be extended.
		
			\begin{definition}[Closed Span]
				The closed span of a subset $A$ of some normed space, e.g. a normed vector space or a Hilbert space, is the closure of $\textit{span}\left(A\right)$ with respect to the distance induced by the norm of the space. In the following it is denoted by $\overline{{\textit{span}\left(A\right)}}$.
			\end{definition}
		
			{\color{red} This is verbatim!}
			\begin{definition}[Orthonormal Sequence in a Hilbert Space]
				Let $\{x_n\}$ be a countable collection of elements in a Hilbert space such that every finite subcollection of $\{x_n\}$ is linearly independent. Define $e_1 = \frac{x_1}{\| x_1 \|}$ and $e_i = \frac{v_i}{\| v_i \|}$ for 
				$$v_i = x_i - \sum_{j = 1}^{i - 1}\langle x_i, e_j\rangle e_j.$$
				Then, $\{e_n\}$ is an orthonormal sequence and $\overline{{\textit{span}\left(\{x_n\}\right)}} = \overline{{\textit{span}\left(\{e_n\}\right)}}$
			\end{definition}
		
			The definition of the closed span and orthonormal sequences makes it possible to define the analogon of the basis in finite dimensional vector spaces for the case of potentially infinite-dimensional Hilbert spaces.
		
			{\color{red} This is verbatim!}
			\begin{definition}[Orthonormal Basis of a Hilbert Space]
				An orthonormal sequence $\{e_n\}$ in a Hilbert space $\mathbb{H}$ is called an orthonormal basis of $\mathbb{H}$ if $\overline{{\textit{span}\left(\{e_n\}\right)}} = \mathbb{H}$. Bases like this are typically called Schauder bases to differentiate them from Hamel bases which are often used in the study of vector spaces. The difference between these is that a Schauder basis can represent elements of the corresponding space as infinite sums of its elements, whereas a Hamel basis can only use finite linear combinations.
			\end{definition}
			
			A special class of Hilbert spaces that is used in most contexts that are relevant in functional data analysis are so called separable Hilbert spaces. Their properties will allow significant simplifications in the calculation of specific test statistics that are at the core of \cite{bugni_permutation_2021}.
			\begin{definition}[Separable Hilbert Space]
				A Hilbert space that possesses a countable complete orthonormal basis is called separable Hilbert space.
			\end{definition}
			Using the axiom of choice, it is possible to show that every Hilbert space possesses an orthonormal basis, which will be used in the derivation of an asymptotic distribution for a test statistic later in this thesis.\\
			
			The Hilbert space that is most important in functional data analysis is the space of square integrable functions. However, this leads to the case that often square integrability is assumed by default and it is interesting to consider whether this property is actually necessary.		
			\begin{definition}[Hilbert Space of Square Integrable Functions]
				
				The space of square integrable functions on a closed interval $\mathcal{I}$ together with the norm $\langle f,g\rangle = \int_{\mathcal{I}} f(t)g(t) \mathrm{d}t$ is a Hilbert space.
				A function $f: \mathcal{I} \rightarrow \mathbb{R}$ is called square-integrable if the following condition holds.
				\begin{equation}
					\int_{A} \left[f(t)\right]^2\mathrm{d}t < \infty
				\end{equation}
				To give the space the properties that are typically desired in Functional data analysis, it is typically defined as a space of equivalence classes, where to functions are seen as equivalent if they differ at most on a set of Lebesgue-measure zero. The Hilbert space of all square integrable functions on $\mathcal{I}$ is denoted by $\mathbb{L}_2(\mathcal{I})$.
			\end{definition}
			
			In most cases, $A$ is chosen as a closed interval of $\mathbb{R}$. Without loss of generality, we can reduce our treatment to the case of $\mathcal{I} = [0,1]$.
			
			\subsubsection{$\mathbb{L}_2(\mathcal{I})$ as defined in \cite{bugni_permutation_2021}}
			Deviating from the norm in functional data analysis, \cite{bugni_permutation_2021} define two square-integrable functions to be distinct even if they differ only on a set of Lebesgue-measure zero. To distinguish between the typical case presented in the previous section, let $\mathbb{L}_2(\mathcal{I})$ denote the Hilbert space of square-integrable functions and $\mathbb{L}^{*}_2(\mathcal{I})$ the square-integrable functions under the the convention from \cite{bugni_permutation_2021}.\\
			
			Using $\mathbb{L}^{*}_2(\mathcal{I})$ creates some interesting theoretical challenges, as the resulting object is in fact not a Hilbert space. To understand the theoretical problems that can occur, I first introduce some additional concepts to illustrate the challenges.
			\begin{definition}[Norm and Seminorm]
				A function $p : \mathbb{V} \rightarrow \mathbb{F}$ on a vector space $\mathbb{V}$ over a field $\mathbb{F}$ is called a norm if the following four conditions hold for all $v,u \in \mathbb{V}$ and $s \in \mathbb{F}$.
				\begin{multicols}{2}
					\begin{enumerate}
						\item $p(v + u) \leq p(v) + p(u)$
						\item $p(sv) = |s| p(v)$
						\item $p(v) \geq 0$
						\item $p(v) = 0 \implies v = 0$
					\end{enumerate}
				\end{multicols}
				If $p : \mathbb{V} \rightarrow \mathbb{F}$ fulfills only properties (1) to (3) it is called a seminorm.
			\end{definition}
			
			In the same way a norm induces a distance on its corresponding normed vectorspace, a seminorm $p$ induces a so-called pseudometric $d$. It is given by $d(v,u) = p(u-v)$.
			
			{\color{red} This is from Wikipedia!!!}
			\begin{definition}[Pseudometric Space]
				A pseudometric space $\left(X, d\right)$ is a set $X$ together with a function $d:X\times X \rightarrow \mathbb{R}_{\geq 0}$, such that $\forall x,y,z \in X$ the following properties hold.
				\begin{multicols}{2}
					\begin{enumerate}
						\item $d(x,x) = 0$
						\item $d(x,y) = d(y,x)$
						\item $d(x,z) \leq d(x,y) + d(y,z)$
					\end{enumerate}
				\end{multicols}
				Therefore, deviating from a metric space, two distinct points in a pseudometric space can have a distance of zero $d(x,y) = 0$ for $x \neq y$.
			\end{definition}
			
			That $\mathbb{L}^{*}_2(\mathcal{I})$ is not a Hilbert space becomes clear, when checking the for the properties of the norm induced by the inner product $\| v \| = \sqrt{\langle v, v\rangle}$.
			One of the properties that has to be fulfilled by a norm is $\| v \| = 0 \Longleftrightarrow v = 0$.			
			Let $f:[0,1] \rightarrow \mathbb{R}$ be given by $f(x) = \mathbbm{1}\left[x = 0.5\right]$. Then we can evaluate the following expression to create a contradiction to the norm properties.
			\begin{equation}
				\| f \| = \sqrt{\langle f, f\rangle} = \sqrt{\int_{0}^{1} \left[f(t)\right]^2\mathrm{d}t } = 0
			\end{equation}
			As $f$ is not the zero element of this space, this is a violation of positive definiteness. Positive definiteness applied to the case at hand, states that $\forall v \in \mathbb{L}_2\left(\mathcal{I}\right) \| v \| = 0 \implies v(x) = 0 \quad \forall x \in \mathcal{I}$  $\forall v \in $. Instead, $\| v \| = \sqrt{\langle v, v\rangle}$ is a seminorm and the defined space should more correctly be treated as a pseudometric space.\\
			
			{\color{red} This is from Wikipedia!!!}
			\begin{definition}[Hausdorff Space]
				A Hausdorff space is a topological space where for any two distinct points $x$ and $y$, there exist a neighborhood $U$ of $x$ and a nieghborhood $V$ of $y$ such that $U$ and $V$ are disjoint. This property is also called neighborhood-separability.
			\end{definition}
			
			One problem of $\mathbb{L}^{*}_2(\mathcal{I})$ is, that if we give the space the topology induced by the obvious seminorm, the resulting space would not be Hausdorff. Thus, limits in the later part of \cite{bugni_permutation_2021} would not be defined.
			A second problem is, that it is not clear how a Schauder basis would be defined for a pseudometric space such as $\mathbb{L}^{*}_2(\mathcal{I})$ and that typical existence results for orthonormal bases might not be available.\\			
			
			In the following, I will therefore restrict my analysis to {\color{red} Hier weiterschreiben!}

			
			
		\subsection{Bases of $\mathbb{L}_2$}\label{bases_L2}
			
			One commonly used orthonormal basis of $\mathbb{L}^2([0,1])$ is the Fourier Basis. It consists of a series of functions $\left(\psi_{i}^{F}(x)\right)_{i \in \mathbb{N}}$ taken from the terms of the sine-cosine form of the Fourier series.
			\begin{equation}
				\psi_{i}^{F}(x) = 
				\begin{cases}
					1 & \text{if} \quad i = 1\\
					\sqrt{2} \cos(\pi i x) & \text{if} \quad i \quad \text{is even} \\
					\sqrt{2} \sin(\pi (i-1)x) & \text{otherwise}
				\end{cases}
			\end{equation}
			Figure \ref{fourier_basis} shows the first seven Fourier basis functions on $[0,1]$.
			\begin{figure}[H]
				\includegraphics[width = \textwidth]{../Graphics/fourier\_basis.PDF}
				\caption{The first seven Fourier basis functions}
				\label{fourier_basis}
			\end{figure}
			A proof that the Fourier basis is in fact an orthonormal basis of $\mathbb{L}^2([0,1])$ can be found in section 2.4 of \cite{hsing_theoretical_2015}. As the Fourier basis is a countable orthonormal basis of $\mathbb{L}^2([0,1])$, we can follow that $\mathbb{L}^2([0,1])$ is a separable Hilbert space, which will be useful in further parts of this thesis.
	
		\subsection{Random Functions}
			Random functions are a special case of general random variables. To understand their connection to the general concepts it is therefore useful to remind ourselves of the definition of a random variable. Paraphrasing from \cite{bauer_probability_2011} this can take the following form.
			\begin{definition}[Random Variable]
				Let $\left(\Omega, \mathcal{A}, \mathcal{P}\right)$ be a probability space and $\left(\Omega', \mathcal{A}'\right)$ be a measure space. Then every $\mathcal{A}$-$\mathcal{A}'$-measurable function $X:\Omega \rightarrow \Omega'$ is called a $\left(\Omega', \mathcal{A}'\right)$-random variable.
			\end{definition}
		
			\begin{definition}[Random Function]
				A random variable that realizes in a function space, e.g. $\mathbb{L}^2[0,1]$, is called a random function.
			\end{definition}
		
		\subsection{Probability Measures on $\mathbb{L}_2$}\label{prob_measures_l2}
			In later parts of this thesis, it will be important to evaluate expectations of a functional on $\mathbb{L}_2$. For this to make sense, it is necessary to explore how we can define probability measures on function spaces. However, due to the way probability measures on $\mathbb{L}_2[0,1]$ are constructed in the paper, I will restrict my description to probability measures that are induced by existing random functions. Assume therefore that $Z(t)$ is a random function realizing in $\mathbb{L}_2[0,1]$ that induces a measure $\mu$. Then, $Z(t)$ can be expressed in terms of a deterministic basis of $\mathbb{L}_2[0,1]$, e.g. the Fourier basis, in the following way.
			\begin{equation}
				Z(t) = \sum_{k = 1}^{\infty} b_k \psi_k(t) \quad \text{s.t.} \quad \sum_{k = 1}^{\infty} b_k^2 \leq \infty \quad \text{a.s.}
			\end{equation}
			Here, the Fourier coefficients $b_k$ are a sequence of scalar random variables that are square summable with probability one. The latter property is needed to ensure square summability of the corresponding random function. The random function is therefore fully described by distribution of a countably infinite, almost surely square-summable sequence of scalar random variables $b_k$. The measure $\mu$ associated with the random variable is therefore equivalent to the measure on the countably infinite space, that the sequence of Fourier coefficients realizes in.\\
			The theory of random sequences is out of the scope of this thesis. However, to understand the principle of how a random variable induces a measure, it suffices to look at a rather basic fact from probability theory. Paraphrasing from \cite{bauer_probability_2011}, let $X:\left(\Omega, \mathcal{A}, \mathcal{P}\right) \rightarrow \left(\Omega', \mathcal{A}'\right)$ be a random variable, then a measure on $\mathcal{A}'$ is induced by $X$ as shown in Equation \ref{induced_measure}.
			\begin{equation}\label{induced_measure}
				\mathcal{P}_X(B) = \mathcal{P}(X \in B) = \mathcal{P}(X^{-1}(B)) \quad \forall B \in \mathcal{A}'
			\end{equation}
			This idea specializes to the case of random functions realizing in $\mathbb{L}_2[0,1]$ and allows us to use measures induced by existing random variables.
			For the purposes of this paper, it is interesting to look at the relationship between the measure induced by the infinite sum shown above and the measure induced by its finite counterpart shown below.
			\begin{equation}
				Z_K(t) = \sum_{k = 1}^{K} b_k \psi_k(t)
			\end{equation}
			\cite{bugni_goodness--fit_2009} show that the measure $\mu_K$ induced by this finite version converges in the following sense to the measure $\mu$ induced by the infinite version. Let $A$ be a $\mu$-measurable subset of $\mathbb{L}_2[0,1]$. Then $\forall a \in A$, there is a unique countably infinite sequence of Fourier coefficients $b(a) = (b_k(a))_{k \in \mathbb{N}}$ given by $b_k(a) = \int_{0}^{1} a(t) \psi_k(t) \mathrm{d}t$. As $a \in \mathbb{L}_2[0,1]$ these coefficients necessarily fulfill $\sum_{k = 1}^{\infty} b_k^2 < \infty$. Using the following definitions,
			\begin{itemize}
				\item $\textbf{B} = \left\{b(a) \ \vert \ a \in A\right\}$
				\item $\textbf{B}_K = \left\{\left(b_1, \dots, b_K \right) \ \vert \ \exists b' \in B \  \forall k = 1, \dots, K \quad b_k = b'_k \right\}$
				\item $\textbf{A}_K = \left\{ \sum_{i = 1}^{\infty} b_k \psi_k(t) \ \vert \ \left(b_1, \dots, b_K \right) \in \textbf{B}_K \quad \text{and} \quad \sum_{k = 1}^{\infty} b_k^2 < \infty \right\}$
			\end{itemize}
			the authors show that for any $A$ in the Borel sigma field of subsets of $\mathbb{L}_2[0,1]$, the following condition holds, where $\mathcal{P}$ denotes the probability measure associated with $\mu$.
			\begin{equation}
				\lim_{K \rightarrow \infty} \mathcal{P}(\textbf{A}_K) = \mathcal{P}(\textbf{A})
			\end{equation}
			In later steps of this thesis, this argument allows us to use a truncation of the basis in the construction of a measure at a reasonably high truncation parameter as integrals with respect to the  measure converge to their counterpart induced by the random variable constructed using a non-truncated basis as the truncation parameter goes to infinity.
			\begin{equation}
				\lim_{K \rightarrow \infty} \int_{\mathbb{L}_2(\mathcal{I})} \mathcal{F}(z) \mathrm{d}\mu_K = \int_{\mathbb{L}_2(\mathcal{I})} \mathcal{F}(z) \mathrm{d}\mu
			\end{equation}
			
			As a more general treatment of these aspects would be mathematically very involved, this section shall primarily serve to give intuition. For readers that are interested in a more detailed mathematical analysis of the described testing procedure, \cite{gihman_theory_2004} and \cite{skorohod_integration_1974} give a rigorous treatment of the necessary theory on measures, probabilities and integration in Hilbert spaces. 
		
		\subsection{Functional Integration on $\mathbb{L}_2$}\label{Integration}
		
			In one of the test statistics used in \cite{bugni_permutation_2021}, it is necessary to integrate over a function space. Therefore, it is necessary to explore the ideas of functional integration and integration on separable Hilbert spaces. 
			The formal definition of a functional integral over the functional $G[f]$ over some separable Hilbert space of functions is given as follows. 
			\begin{equation}
				\int_{\mathbb{L}_2(\mathcal{I})} G\left[f\right] \left[Df\right] = \int_{-\infty}^{\infty}\dots\int_{-\infty}^{\infty} G\left(f_1, f_2, \dots\right) \prod_{n} \mathrm{d}f_n
			\end{equation}
		
			However, as the test statistic is in reality always calculated using Monte-Carlo integration, it is more useful to give a brief theoretical overview of functional integration and to instead introduce Monte-Carlo method with respect to some measure induced by a random variable. Similar to the more general concept of Monte-Carlo simulation, Monte-Carlo integration relies on randomness to approximate an object that might be difficult to evaluate exactly.\\
			For simplicity, assume that we want to approximate a 
		
			As in the previous section, an in-depth treatment of integration on Hilbert spaces is out of the scope of this thesis. However, a detailed overview on integration on Hilbert spaces is given in \cite{skorohod_integration_1974} and could be used to derive theoretical properties of the test in a more general setting.
		
	\section{Cram\'{e}r-von Mises Tests}\label{CvM_Tests}
		In applied econometrics, it is often interesting to ask whether the same stochastic process generated the observations in two distinct data sets. In an experimental setting, we could ask whether a treatment assigned at random to a subset of agents changed the distribution of an outcome variable. 
		One approach to answering this question is given by the two-sample Cram\'{e}r-von Mises test.
		
		\begin{itemize}
			\item \cite{darling_kolmogorov-smirnov_1957}
			\item \cite{anderson_asymptotic_1952}
			\item \cite{buning_nichtparametrische_2013}
		\end{itemize}
	
		\subsection{Empirical Distribution Functions}
			
			\cite{gibbons_nonparametric_2021}
			\begin{definition}[Order Statistic]\label{Order_Stat}
				
				Let $\{x_i \: \vert \: i = 1, \dots , n\}$ be a random sample from a population with continuous cumulative distribution function $F_X$. Then there almost surely exists a unique ordered arrangement within the sample. 
				
				$$X_{(1)} < X_{(2)} < \dots < X_{(n)}$$
				
				$X_{(r)} \quad r \in \{1, \dots, n\}$ is called the $r$th-order statistic.	
			\end{definition}
		
			\begin{definition}[Empirical Distribution Function]
				\begin{equation}
					F_{n}(x) = \begin{cases}
						0 & \quad \text{if} \quad  x < x_{(1)} \\
						\frac{r}{n} & \quad \text{if} \quad  x_{(r)} \leq x < x_{(r + 1)} \\
						1 & \quad \text{if} \quad  x \geq x_{(n)}
					\end{cases}
				\end{equation}
			\end{definition}
		
		\subsection{Assumptions}
		
		\subsection{Nullhypothesis}
			Let $\{x_1, \dots , x_n\}$ and $\{y_1, \dots , y_m\}$ be two data sets generated by random variables $X \sim_{\text{i.i.d.}} F(t)$ and $Y \sim_{\text{i.i.d.}} G(t)$.
			Then, we can formulate the Nullhypothesis that both samples were independently generated by random variables following the same distribution function.
			\begin{equation}
				\begin{split}
					H_0&: F(t) = G(t) \quad \forall t \in \mathbb{R}\\
					H_1&: \exists t \in \mathbb{R} \quad \text{s.t.} \quad F(t) \neq G(t)
				\end{split}
			\end{equation}
			
		\subsection{Two-Sample Cram\'{e}r-von Mises Statistic}\label{Two_sample_CvM}
			
			\cite{buning_nichtparametrische_2013}
			\begin{equation}
				C_{m,n} = \left(\frac{nm}{n+m}\right) \int_{-\infty}^{\infty}\left(F_{m}(t) - G_{n}(t)\right)^{2} \mathrm{d} \left(\frac{m F_{m}(t) + n G_{n}(t)}{m+n}\right)
			\end{equation}
			\cite{anderson_distribution_1962} explores the small sample distribution of this test statistic and provides a comparison to the limiting distribution derived by \cite{rosenblatt_limit_1952} and \cite{fisz_result_1960}.
			In addition to this formulation of the test, it is possible to introduce a weight function $w(x)$ in the following way. 
			\begin{equation}
				T^{\textit{EDF}}_{m,n} = \left(\frac{nm}{n+m}\right) \int_{-\infty}^{\infty}\left(F_{m}(t) - G_{n}(t)\right)^{2} w(t) \mathrm{d} \left(\frac{m F_{m}(t) + n G_{n}(t)}{m+n}\right)
			\end{equation}
			Then the Cram\'{e}r-von Mises test is just the special case for $w(t) = 1$. Other weight functions are possible and it is often advisable to think about the desired properties of the test, when choosing a specific weight function as it can have important effects on the power in different scenarios. One of the most typical weight functions for the one-sample case was proposed by \cite{anderson_asymptotic_1952} and \cite{anderson_test_1954} and adapted to the two-sample setting by \cite{pettitt_two-sample_1976}. The latter is given by using the weight function $w(t) = \frac{m F_m(t) + n G_n(t)}{m+n}$ and has been shown to have better properties in some scenarios.
		
		\subsection{Asymptotic Distribution}
			As shown by the previously mentioned authors, under the Nullhypothesis that both samples were independently generated by random variables sharing the same distribution function, we can find the following limiting distribution of $C_{m,n}$.
			\begin{equation}
				\begin{split}
					C_{m,n} &\xrightarrow{\text{d}} \int_{0}^{1} \left(Z(u) + \left(1 + \lambda\right)^{-\frac{1}{2}} f(u) - \left[\frac{\lambda}{1+\lambda}\right]^{\frac{1}{2}}g(u)\right)^2 \mathrm{d}u \\
					\text{as} \quad &n \rightarrow \infty, \quad m \rightarrow \infty, \quad \frac{n}{m} \rightarrow \lambda \in \mathbb{R}
				\end{split}
			\end{equation}
			Here, $Z(u)$ is a Gaussian stochastic process with the following properties.
			\begin{itemize}
				\item $\mathbb{E}\left[Z(u)\right] = 0 \quad \forall u \in [0,1]$
				\item $Cov\left(Z(u), Z(v)\right) = \min(u,v) - uv \quad \forall u,v \in [0,1]$
			\end{itemize}					
			
	\section{Permutation Tests}\label{Permutation_Tests}
		In layman's terms, the idea of a permutation test is the following: if two samples show distinctly different properties, that will lead to differences in an appropriately chosen summary statistic. If we were to permute the elements of the groups randomly, we would expect these differences to disappear.
		Permutation tests formalize this intuition. The following section closely follows chapter 15 from \cite{lehmann_testing_2005}.
		
		\begin{itemize}
			\item \cite{van_der_vaart_weak_1996}
		\end{itemize}
	
		\subsection{Functional Principle of Permutation Tests}
		
			One on the defining features of each test is its Nullhypothesis. For the case of randomization tests, we can formulate it quite generally. Let $X$ be data taking values in a sample space $\mathcal{X}$. Then, the hypothesis is that the probability law $P$ generating $X$ belongs to a family of distributions $\Omega_0$. \cite{lehmann_testing_2005} define an assumption called the randomization hypothesis that allows for the construction of randomization tests. As permutation tests are a special case of randomization tests, we can specialize this definition to the case under consideration.
			\begin{assumption}[Randomization Hypothesis]\label{rand_hypo}
				 Let $G$ be a finite group of transformations $g: \mathcal{X} \rightarrow \mathcal{X}$. Under the Nullhypothesis of the randomization test, the distribution of $X$ is invariant under the transformations $g \in G$. In other words, $gX$ and $X$ have the same distribution whenever $X$ has distribution $P \in \Omega_0$.
			\end{assumption}
			Under this assumption one can construct a permutation test based on any test statistic $T:\mathcal{X} \rightarrow \mathbb{R}$ that is suitable to test the Nullhypothesis under consideration. Suppose that $G$ has $M$ elements, then given $X = x$, let 
			$$T_{(1)}(x) \leq T_{(2)}(x) \leq \dots \leq T_{(M)}(x) $$
			be the ordered values of the test statistic $T(gx)$ as described in Definition \ref{Order_Stat} as $g$ varies over $G$. For a fixed nominal level $\alpha \in (0,1)$, define $k = M - \lfloor M\alpha \rfloor$. Additionally define the following two objects.
			\begin{multicols}{2}
				\noindent
				\begin{equation*}
					M^{+} = \sum_{m = 1}^{M}\mathbbm{1}\left[T_{(m)}(x) > T_{(k)}(x)\right]
				\end{equation*}
				\begin{equation}
					M^{0} = \sum_{m = 1}^{M}\mathbbm{1}\left[T_{(m)}(x) = T_{(k)}(x)\right]
				\end{equation}
			\end{multicols}
			
			\begin{definition}[Randomization Test Function]\label{RandTestFunc}
				We define the Randomization Test Function as the following function $\phi: \mathcal{X} \rightarrow \mathbb{R}$.
					\begin{equation*}
						\phi(x) = \begin{cases}
							1 &\text{if} \ T > T_{(k)}(x) \\
							a &\text{if} \ T = T_{(k)}(x) \\
							0 &\text{if} \ T < T_{(k)}(x) \\
						\end{cases} \quad \text{where} \quad
						a = \frac{M\alpha - M^{+}(x)}{M^{0}(x)}
					\end{equation*}
				
			\end{definition}
			This function can be interpreted as a decider of whether a Null hypothesis is rejected. If $\tau(X) = 1$, the Null hypothesis is rejected, if $\tau(X) = 0$ it cannot be rejected, and if $\tau(X) = a$, the decision is randomized using a Bernoulli variable that is $1$ with probability $a$. 
			Under Assumption \ref{rand_hypo}, it is possible to show that, given a test statistic $T = T(X)$, the resulting test $\phi$ has size $\alpha$.
			\begin{equation}
				\mathbb{E}_{P}\left[\phi(X)\right] = \alpha \quad \forall P \in \Omega_0
			\end{equation}
			This effectively means, that a permutation test constructed in the described way, always has the correct specified size. However, the power against alternatives can be highly sensitive to both the sample sizes and the number of permutations that was used to derive the permutation distribution.			
		
			\cite{lehmann_testing_2005} explore the example of testing for the equality of the generating probability laws of two independent samples. This is precisely the relevant application for the permutation variant of the two-sample Cram\'{e}r-von Mises test that \cite{bugni_permutation_2021} extend to the setting of functional data. \\
			
			\begin{definition}[Permutation]
				Let $S$ be a set, then a permutation of $S$ is a bijective function $\pi: S \rightarrow S$.
			\end{definition}
			If $S$ is a finite set with $N$ elements, there are $N!$ different permutations. If we apply this idea to the setting of two samples with $n$ and $m$ observations respectively, there are $(n+m)!$ permutations in the combined set of observations. One way of describing the corresponding group of transformations $G$ is shown in Equation \ref{Permutations}.
			\begin{equation}\label{Permutations}
				\begin{split}
					\Pi_N &= \left\{\pi: \left\{1, \dots, N \right\} \rightarrow \left\{1, \dots, N \right\} \ \vert \ \pi \ \text{is bijective} \right\} \\
					G &= \left\{g:\mathbb{R}^N \rightarrow \mathbb{R}^N \ \vert \ \exists \pi \in \Pi_N \ \forall x \in \mathbb{R}^N \ g(x) = \left(x_{\pi(1)}, \dots, x_{\pi(N)}\right) \right\}
				\end{split}
			\end{equation}
			
		 
			Number of Combinations: $\binom{m+n}{m}$\\
			
			For my implementation of the test described in \cite{bugni_permutation_2021}, I chose the latter variant and focused on distinct combinations if it was feasible to calculate the test statistic for all combinations. If this approach was infeasible due to the number of combinations, I resorted to randomly sampling permutations.\\
			
	
		\subsection{Size and Power}
		
		\subsection{Asymptotic Properties}
		
	\section{Test by Bugni and Horowitz (2021)}\label{Bugni_Horowitz_2021}
	
		Similar to the idea of the Cram\'{e}r-von Mises tests in a scalar setting, \cite{bugni_permutation_2021} devise a permutation test for the equality of the distributions of the data generating processes of two independent samples of functional data. 
		To define the exact hypothesis, it is therefore necessary to define a distribution function for a random variable realizing in $\mathbb{L}_2(\mathcal{I})$.
		\begin{definition}[Distribution Function of a Random Function]
			Let $X:\Omega \rightarrow \mathbb{L}_2(\mathcal{I})$ be a random function realizing in the square-integrable functions. Then its distribution function is defined as the following object.
			\begin{equation*}
				F_X(z) = \mathbb{P}\left[X(t) \leq z(t) \quad \forall t \in \mathcal{I}\right] \quad z \in \mathbb{L}_2(\mathcal{I})
			\end{equation*}
		\end{definition}
		Deviating from the norm in functional data analysis, the authors assume that two functions $z_1, z_2 \in \mathbb{L}_2(\mathcal{I})$ are distinct even if they only differ on a set of Lebesgue-measure zero.\\
		
		Tests of this nature exist in some forms already and the following list shall give a non-exhaustive overview of other procedures that could be applied.
		\begin{itemize}
			\item \cite{hall_permutation_2002}
			\item \cite{hall_two-sample_2007}
			\item \cite{pomann_two-sample_2016}
			\item {\color{red} Andere Tests auffuehren}
		\end{itemize}
		
		\subsection{Assumptions}
		As for most tests, there are assumptions that are necessary for the test to work correctly. The authors explicitly make four assumptions, some of which can be relaxed to allow for a more general setting.
			\begin{assumption} Contains two assumptions
				\begin{enumerate}
					\item $X(t)$ and $Y(t)$ are separable, $\mu$-measurable stochastic processes.
					\item $\{X_i(t) \: \vert \: i = 1, \dots, n\}$ is an independent random sample of the process $X(t)$. \\
					$\{Y_i(t) \: \vert \: i = 1, \dots, m\}$ is an independent random sample of $Y(t)$ and is independent of $\{X_i(t) \: \vert \: i = 1, \dots, n\}$.
				\end{enumerate}
			\end{assumption}
		
			\begin{definition}[Separable Stochastic Process]
				{\color{red} From Wikipedia:} A real-valued continuous time stochastic process $X$ with a probability space $\left(\Omega, \mathcal{F}, \mathcal{P}\right)$ is separable if its index set $T$ has a dense countable subset $U \subset T$ and there is a set $\Omega_0 \subset \Omega$ of probability zero, so $\mathcal{P}\left(\Omega_0\right) = 0$, such that for every open set $G \subset T$ and every closed set $F \subset \mathbb{R}$ the two events $\left\{X_t \in F \quad \forall t \in G \cap U\right\}$ and $\left\{X_t \in F \quad \forall t \in G\right\}$ differ from each other at most on a subset $\Omega_0$.
			\end{definition}
		
			In less theoretical terms this means that the process is determined by its values on a countable subset of points of its index set.
		
			\begin{assumption}
				$\mathbb{E}X(t)$ and $\mathbb{E}Y(t)$ exist and are finite for all $t \in [0, T]$.
			\end{assumption}
		
			\begin{assumption}\label{continuous_observation}
				$X_i(t)$ and $Y_i(t)$ are observed for all $t \in \mathcal{I}$.
			\end{assumption}
			Assumption \ref{continuous_observation} can be relaxed and a similar test can be constructed for the case of discretely observed processes. This variation of the test will not be addressed in this thesis. However, \cite{bugni_permutation_2021} provide a description of how to extend their idea to this common scenario.
	
		\subsection{Nullhypothesis}
			As with most permutation tests, the null hypothesis of the test presented by \cite{bugni_permutation_2021} is distributional in nature and is formulated by the authors as follows.
			\begin{equation}
				\begin{split}
					H_0: \quad &F_X(z) = F_Y(z) \quad \forall z \in \mathbb{L}_2(\mathcal{I}) \\
					H_1: \quad &\mathbb{P}_{\mu}\left[F_X(Z) \neq F_Y(Z)\right] > 0
				\end{split}
			\end{equation}
			Here, $\mu$ is a probability measure on $\mathbb{L}_2(\mathcal{I})$ and $Z$ is a random function with probability distribution $\mu$. As can be seen by the formulation of the hypothesis, this test does not intend to have power against alternatives that differ only on a set of functions that have $\mu$-measure zero. Thereby, the choice of the measure is an important factor to consider when trying to use this test against a specific suspected alternative.  
		
		\subsection{Cram\'{e}r-von Mises type Test}
		
			Empirical Distribution Functions
			\begin{multicols}{2}
				\noindent
				\begin{equation*}
					\hat{F}_X(z) = \frac{1}{n} \sum_{i = 1}^{n}\mathbbm{1}\left[X_i(t) \leq z(t) \ \forall t \in \mathcal{I}\right]
				\end{equation*}
				\begin{equation}
					\hat{F}_Y(z) = \frac{1}{m} \sum_{i = 1}^{m}\mathbbm{1}\left[Y_i(t) \leq z(t) \ \forall t \in \mathcal{I}\right]
				\end{equation}
			\end{multicols}
			
			
			Test statistic
			\begin{equation}
				\tau = \int_{\mathbb{L}_2(\mathcal{I})}\left[F_X(z) - F_Y(z)\right]^2 \mathrm{d} \mu(z)
			\end{equation}
			
			Sample analog:
			\begin{equation}
				\tau_{n,m} = (n+m) \int_{\mathbb{L}_2(\mathcal{I})}\left[\hat{F}_X(z) - \hat{F}_Y(z)\right]^2 \mathrm{d} \mu(z)
			\end{equation}
		
			As algebraic functional integration is not suitable for this testing procedure due to the structure of the underlying objects, the authors use Monte-Carlo integration as explained in Section \ref{Integration} for the approximation of this integral. To perform the test, it is therefore necessary to choose a parameter $L$ that determines the number of functions $\left\{Z_l \ \vert \ l = 1, \dots, L\right\}$ that are used to approximate the value of $\tau_{n,m}$ by the following equation.
			\begin{equation}
				\hat{\tau}_{n,m} = \frac{n+m}{L} \sum_{l = 1}^{L} \left[\hat{F}_X(Z_l) - \hat{F}_Y(Z_l)\right]^2
			\end{equation}
			These functions are drawn from a random function associated with the chosen measure $\mu$ and their construction is explained in more detail in Section \ref{mu}.
			 {\color{red} Hier weiterschreiben}
		
			Critical values for Permutation Test Statistic
			\begin{equation}
				t^{*}_{n,m}(1-\alpha) = \inf \left\{t \in \mathbb{R} \quad \vert \quad \frac{1}{Q} \sum_{i = 1}^{Q} \mathbbm{1}\left[\tau_{n,m,q} \leq t\right] \geq 1 - \alpha \right\}
			\end{equation}
		
		\subsection{Asymptotics for the Cram\'{e}r-von Mises type Test}
			Similar to the case presented in \cite{bugni_goodness--fit_2009}, we can derive an asymptotic distribution for the Cram\'{e}r-von Mises type test. Even though this is not necessary to perform the described permutation test, it is an interesting benchmark to compare the test procedure. \\
			
			Under the nullhypothesis and mild regularity conditions, it is possible to show that 
			\begin{equation}
				(n+m)^{\frac{1}{2}} \left[\hat{F}_X(z) - \hat{F}_Y(z)\right]
			\end{equation}
			converges to a mean-zero Gaussian process. For the derivation look in Appendix \ref{asymp_deriv}.
		
		\subsection{Construction of the Measure $\mu$}\label{mu}
			Another component of the test that has to be chosen is the probability measure with respect to which the hypothesis is formulated. This probability measure has a similar function to the weight function described in Section \ref{Two_sample_CvM} in that it determines against which alternatives the test has comparatively high power. It is therefore important to choose the measure according to the kind of violation of the Nullhypothesis that is suspected. If no specific type of violation is expected, a choice similar to a constant weight function can be made.\\
			
			For the calculation of the Cram\'{e}r-von Mises type test, we need to construct one such probability measure that is suitable to detect the kind of alternative we expect to find.
			As hinted at in Section \ref{prob_measures_l2}, \cite{bugni_permutation_2021} approach this problem by constructing a random function that induces a suitable probability measure. Its construction is shown below.
			
			\begin{equation}\label{non_truncated}
				Z(t) = \sum_{k = 1}^{\infty} b_k \psi_k(t)
				\quad \text{s.t.} \quad
				\sum_{k = 1}^{\infty} b_k^2 < \infty \quad \text{a.s.}
			\end{equation}

			\begin{equation}\label{truncation}
				Z_K(t) = \sum_{k = 1}^{K} b_k \psi_k(t)
			\end{equation}
		
			\begin{equation}
				\mathbb{E}\left[Z_K(t)\right] = \sum_{k = 1}^{K} \mathbb{E}\left[b_k\right] \psi_k(t)
				\quad \text{where} \quad
				\mathbb{E}\left[b_k\right] = \int_{\mathcal{I}}w(t)\psi_k(t) \mathrm{d}t
			\end{equation}
		
			\begin{equation}\label{Fourier_coefs}
				b_k = \mathbb{E}\left[b_k\right] + \rho_k U_k
				\quad \text{s.t.} \quad
				\sum_{k = 1}^{\infty} \rho_k^2 < \infty 
			\end{equation}

			To obtain a reasonable degree of power it is important to set the expected value of the random function $Z(t)$ in a region, where data is observed and optimally in a region where realizations of the random function inducing the measure $\mu$ are able to distinguish potential differences in the samples. This could for example be done by choosing $w(t)$ dependent on the data. 	The authors suggest to choose $w(t):\mathcal{I} \rightarrow \mathbb{R}$ to be comparatively large in the parts of $\mathcal{I}$ where possible differences between the empirical distribution functions are expected to be large.
			Taking the option used for the application in the original paper and one further possibility, the following two ways of choosing the mean function might come to mind.
			
			\begin{enumerate}
				\item In their application \cite{bugni_permutation_2021} set the distribution of the fourier coefficients manually, as $b_1 \sim \mathcal{N}(\mu_1, \frac{1}{K})$ and $b_k \sim \mathcal{N}(0, \frac{1}{K}) \quad k = 2, \dots, K$ where $\mu_1 = \textit{median}_i \left(\textit{max}_t\left\{X_i(t) \ | \ i = 1, \dots, n_1, t\in \mathcal{I}\right\}\right)$.
				This method is identical to setting $w(t) = \mu_1(t)$, as the inner product of any constant function with Fourier basis functions of order two or higher will be zero due to their cyclical nature.
				\item Another idea is to choose a non-constant weight function based on the reference sample. One option for this choice is to use specific point-wise quantiles of the reference sample: $w(t) = \textit{quantile}_q \{X_i(t) \ | \ i = 1, \dots n_1\}$. This allows the generated functions for the Monte-Carlo integration in the approximation of the Cram\'{e}r-von Mises statistic to resemble the reference data more closely, which could improve the properties of the test.
			\end{enumerate}
		
			\begin{figure}[H]
				\makebox[\textwidth][c]{
					\includegraphics[width = 1.1\textwidth]{../Graphics/mean\_functions.PDF}
				}
				\caption{Mean Functions calculated for a Reference Sample as described in Section \ref{Simulation_Study} for the 95\% Quantile.}
				\label{mean_functions}
			\end{figure}
			
			
			Additionally, the choice of the sequence $(\rho_k)_{k \in \mathbb{N}}$ is of importance. The authors do not recommend a procedure for choosing these coefficients, but it seems reasonable to choose parameters that lie in the vicinity of the standard deviations for the Fourier coefficients of the reference sample. This method might be a reasonable possibility to choose these parameters dependent on the data.\\
			
			In \cite{bugni_goodness--fit_2009}, the authors show that the approximation of the probability measure, which is induced by the approximation of the random variable in Equation \ref{truncation}, converges appropriately to the probability measure induced by the random variable in Equation \ref{non_truncated}. {\color{red} Hier weiterschreiben}\\
			
			At this point it is again interesting to look at the convention that two elements of $\mathbb{L}_2(\mathcal{I})$ are distinct even if they differ only on a nonempty set of Lebesgue-measure zero. One problem that this convention entails is the fact that the Fourier basis is an almost everywhere basis of $\mathbb{L}_2\left(\mathcal{I}\right)$ as shown by \cite{carleson_convergence_1966}. However, in many cases point-wise convergence of a sum as shown in Equation \ref{non_truncated} is not fulfilled. This is simple to see for any function that does not have identical values on both sides of its domain. {\color{red} Hier weiterschreiben}\\
			
			In more basic terms, this implies that using the Fourier basis, it is impossible to construct some functions in $\mathbb{L}^{*}_2\left(\mathcal{I}\right)$ even using a non-truncated representation as shown in Equation \ref{non_truncated}.			
			In a typical scenario where we would use $\mathbb{L}_2\left(\mathcal{I}\right)$, so only their equivalence classes of the functions, this does not pose a problem. 
			This in turn implies that a probability measure that is constructed as shown in the previous section cannot give positive weight to many functions in the space $\mathbb{L}_2(\mathcal{I})$ if we use the convention used by the authors. Namely, for any function in $\mathbb{L}_2(\mathcal{I})$ for which the Fourier series fails to converge point-wise, we cannot assign a positive probability. This in turn could be a potential problem for the method described in the paper, if this is necessary for its working principle.
			
		\subsection{Mean focused Test}
			In addition to the Cram\'{e}r-von Mises type test, the authors use a second test statistic aimed at detecting violations due to a mean shift in the data generating process. This test is added due to low power against pure mean-shift alternatives in the original paper's simulations and is supposed to complement this weakness of the other test statistic.
			
			The variable $\mu$ on which this second test is based is the distance between the mean functions of the data generating processes as induced by the natural norm on $\mathbb{L}_2[0,1]$.
			\begin{equation}
				\nu = \int_{\mathcal{I}} \big(\mathbb{E}\left[X(t)\right] - \mathbb{E}\left[Y(t)\right]\big)^2 \mathrm{d}t
			\end{equation}
		
			Mean Estimators
			\begin{multicols}{2}
				\noindent
				\begin{equation*}
					\hat{\mathbb{E}}\left[X(t)\right] = \frac{1}{n}\sum_{i = 1}^{n} X_i(t)
				\end{equation*}
				\begin{equation}
					\hat{\mathbb{E}}\left[Y(t)\right] = \frac{1}{m}\sum_{i = 1}^{m} Y_i(t)
				\end{equation}
			\end{multicols}
		
			Sample Analog
			\begin{equation}
				\nu_{n,m} = (n+m) \int_{\mathcal{I}} \left[\hat{\mathbb{E}}X(t) - \hat{\mathbb{E}}Y(t)\right]^2 \mathrm{d}t
			\end{equation}
		
			Critical values for Permutation Test Statistic
			\begin{equation}
				t^{*}_{n,m}(1-\alpha) = \inf \left\{t \in \mathbb{R} \quad \vert \quad \frac{1}{Q} \sum_{i = 1}^{Q} \mathbbm{1}\left[\nu_{n,m,q} \leq t\right] \geq 1 - \alpha \right\}
			\end{equation}
		
		\subsection{Combined Permutation Test}
			Define for the two underlying tests the following permutation test functions as described in Definition \ref{RandTestFunc} for the general case of a randomization test.
			\begin{multicols}{2}
				\noindent
				\begin{equation*}
					\phi_{n,m} = \begin{cases}
						1 &\text{if} \ \tau_{n,m} > t^{*}_{n,m}(1-\alpha_{\tau}) \\
						a_{\tau} &\text{if} \ \tau_{n,m} = t^{*}_{n,m}(1-\alpha_{\tau}) \\
						0 &\text{if} \ \tau_{n,m} < t^{*}_{n,m}(1-\alpha_{\tau}) \\
					\end{cases}
				\end{equation*}
				\begin{equation}
					\tilde{\phi}_{n,m} = \begin{cases}
						1 &\text{if} \ \nu_{n,m} > t^{*}_{n,m}(1-\alpha_{\nu}) \\
						a_{\nu} &\text{if} \ \nu_{n,m} = t^{*}_{n,m}(1-\alpha_{\nu}) \\
						0 &\text{if} \ \nu_{n,m} < t^{*}_{n,m}(1-\alpha_{\nu}) \\
					\end{cases}
				\end{equation}
			\end{multicols}
			$a_\tau$ and $a_\nu$ are given by the following equations to ensure that the expected values of $\phi$ and $\tilde{\phi}$ have the desired values.
			\begin{multicols}{2}
				\begin{itemize}
					\item $a_{\tau} = \frac{Q\alpha_{\tau} - Q_{\tau}^{+}}{Q_{\tau}^{0}}$ 
					\item $Q_{\tau}^{+} = \sum_{q = 1}^{Q}\mathbbm{1}\left[\tau_{n,m,q} > t^{*}_{n,m}(1-\alpha_{\tau})\right]$
					\item $Q_{\tau}^{0} = \sum_{q = 1}^{Q}\mathbbm{1}\left[\tau_{n,m,q} = t^{*}_{n,m}(1-\alpha_{\tau})\right]$
					\item $a_{\nu} = \frac{Q\alpha_{\nu} - Q_{\nu}^{+}}{Q_{\nu}^{0}}$ 
					\item $Q_{\nu}^{+} = \sum_{q = 1}^{Q}\mathbbm{1}\left[\nu_{n,m,q} > t^{*}_{n,m}(1-\alpha_{\nu})\right]$
					\item $Q_{\nu}^{0} = \sum_{q = 1}^{Q}\mathbbm{1}\left[\nu_{n,m,q} = t^{*}_{n,m}(1-\alpha_{\nu})\right]$
				\end{itemize} 
			\end{multicols}
			
			Bonferroni inequality under $H_0$ leads to
			\begin{equation}
				\max(\alpha_{\tau}, \alpha_{\nu}) \leq \mathbb{P}\left[(\phi_{n,m} > 0) \cup (\tilde{\phi}_{n,m} > 0)\right] \leq \alpha_{\tau} + \alpha_{\nu}
			\end{equation}
		
		\subsection{Finite Sample Properties under the Nullhypothesis}
		For any distribution $P$ that satisfies the Nullhypothesis and any $\alpha_{\tau}, \alpha_{\mu} \in (0,1)$, we have 
		\begin{multicols}{2}
			\noindent
			\begin{equation*}
				\mathbb{E}_P\left(\phi_{n,m}\right) = \alpha_{\tau}
			\end{equation*}			
			\begin{equation}
				\mathbb{E}_P\left(\tilde{\phi}_{n,m}\right) = \alpha_{\nu}
			\end{equation}
		\end{multicols}
		
		\subsection{Asymptotic Properties under the Alternative}
		
	\section{Test for Persistence Alternatives}\label{variant}
		One particularly interesting potential application for the Cram\'{e}r-von Mises type test presented in \cite{bugni_permutation_2021}, is a slight modification leading to the following test for differences in the persistence structure of the data generating processes. 
		There are already many tests focusing on differences in the mean functions of processes. A non-exhaustive list of examples can be found in the following publications.
		\begin{itemize}
			\item \cite{lee_two_2015}
			\item \cite{cox_pointwise_2008}
			\item {\color{red} Hier tests auflisten}
		\end{itemize}
		There is also a large number of tests for differences in the variance functions of processes such as for example the tests presented in the following papers. 
		\begin{itemize}
			\item {\color{red} Hier tests auflisten}
		\end{itemize}
		However, there are few tests that address differences in the persistence of processes and the proposed procedure adds to the limited arsenal.\\
		
		\subsection{Null-Hypothesis}
		Let $\{X_i(t) \ | \  i = 1, \dots, n\}$ and $\{Y_i(t) \ | \  i = 1, \dots, m\}$ again denote the samples under consideration. We assume that the samples consist of i.i.d. realizations of random functions $X(t)$ and $Y(t)$ respectively.
		To focus on the persistence properties of the random functions, we can define the standardized counterparts to these random functions in the following way. Define the following objects.
		\begin{multicols}{2}
			\noindent
			\begin{equation*}
				\begin{split}
					\mu_{X}(t) = &\mathbb{E} X(t) \\
					\sigma_{X}(t) = &\sqrt{\mathbb{E}\left[X(t) - \mu_{X}(t)\right]^2}
				\end{split}
			\end{equation*}
			\begin{equation}
				\begin{split}
					\mu_{Y}(t) = &\mathbb{E} Y(t) \\
					\sigma_{Y}(t) = &\sqrt{\mathbb{E}\left[Y(t) - \mu_{Y}(t)\right]^2}
				\end{split}
			\end{equation}
		\end{multicols}
		Then the standardized random variables can be defined as follows.
		\begin{multicols}{2}
			\noindent
			\begin{equation*}
				\tilde{X}(t) = \frac{X(t) - \mu_{X}(t)}{\sigma_{X}(t)}
			\end{equation*}
			\begin{equation}
				\tilde{Y}(t) = \frac{Y(t) - \mu_{Y}(t)}{\sigma_{Y}(t)}
			\end{equation}
		\end{multicols}
		Their distribution functions are defined analogously to Section \ref{Bugni_Horowitz_2021} and denoted by $F_{\tilde{X}}(z)$ and $F_{\tilde{Y}}(z)$ respectively. Then we can formulate a Null-hypothesis as in \cite{bugni_permutation_2021} with respect to the standardized random functions.
		\begin{equation}
			\begin{split}
				H_0: \quad &F_{\tilde{X}}(z) = F_{\tilde{Y}}(z) \quad \forall z \in \mathbb{L}_2(\mathcal{I}) \\
				H_1: \quad &\mathbb{P}_{\mu}\left[F_{\tilde{X}}(z) \neq F_{\tilde{Y}}(z)\right] > 0
			\end{split}
		\end{equation}
		As all differences in the mean and variance patterns of these processes is eliminated by the standardization, one of the distinct remaining features that might differentiate these processes are their persistence properties. Therefore, as this test in its more general form is supposed to find very general alternatives, this focused approach could be tailored by choice of $\mu$ to detect differences in the persistence structure of the data generating processes. \\
		
		\subsection{Test-Procedure}\label{persistence_test}
		The test is functionally nearly identical to the Cram\'{e}r-von Mises type test described in \cite{bugni_permutation_2021}. However, it adds a standardization step in the calculation of the test statistic, to eliminate differences in the mean functions of the processes and their variance structure. In each permutation step, the calculation of the test statistic is thus described by the following algorithm.\\
		
		Let $\mathcal{W} = \{W_i(t) \ | \ i = 1, \dots, n\}$ be sample 1 and $\mathcal{V} = \{V_i(t) \ | \  i = 1, \dots, m\}$ be sample 2. These samples are generated by randomly permuting the original samples $\{X_i(t) \ | \  i = 1, \dots, n\}$ be sample 1 and $\{Y_i(t) \ | \  i = 1, \dots, m\}$.
		\begin{enumerate}
			\item Calculate the sample mean functions for the permuted samples: \\
			$\bar{W}(t) = \frac{1}{n} \sum_{i = 1}^{n} W_i(t)$ and $\bar{V}(t) = \frac{1}{m} \sum_{i = 1}^{m} V_i(t)$
			\item Center the sample curves to obtain centered permutation samples: \\
			$\bar{\mathcal{W}} = \{W_i(t) - \bar{W}(t) \ | \ i = 1, \dots, n\}$ and $\bar{\mathcal{V}} = \{V_i(t) - \bar{V}(t) \ | \ i = 1, \dots, m\}$
			\item Calculate the point-wise standard deviations: \\
			$\sigma_{\mathcal{W}}(t) = \sqrt{\frac{1}{n}\sum_{i = 1}^{n}\left[W_i(t) - \bar{W}(t)\right]^2}$ and $\sigma_{\mathcal{V}}(t) = \sqrt{\frac{1}{m}\sum_{i = 1}^{m}\left[V_i(t) - \bar{V}(t)\right]^2}$
			\item Standardize the observations in the permuted samples:\\
			$\tilde{\mathcal{W}} = \{\frac{W_i(t) - \bar{W}(t)}{\sigma_{\mathcal{W}}(t)} \ | \ i = 1, \dots, n\}$ and $\tilde{\mathcal{V}} = \{ \frac{V_i(t) - \bar{V}(t)}{\sigma_{\mathcal{V}}(t)} \ | \ i = 1, \dots, m\}$
			\item Calculate $\hat{\tau}_{n,m}$ for the standardized permutation samples $\tilde{\mathcal{W}}$ and $\tilde{\mathcal{V}}$ as described in Section \ref{Bugni_Horowitz_2021} for a chosen measure $\mu$.
		\end{enumerate}
		The overarching test procedure is again given by a derivation of a permutation distribution and a permutation test decision according to the rules stated in Section \ref{Permutation_Tests}.\\
		
		Due to the introduction of a standardization step after the permutation of the samples, the precise order of operations chosen for the implementation is now of considerable importance if the sample does not actually consist of continuously observed curves but of sequences of discrete observations sharing the points where observations are made. This could for example mean a setting where at distinct points in time, measurements are made for a group of individuals.
		\begin{enumerate}
			\item The intuitive approach to standardizing observations like this would be to standardize the discrete observation points in each permutation step and to fit a functional basis to the discrete sequence of standardized observations.
			\item Another approach would be to fit the functional basis to the non-standardized samples and to standardize on the level of the fitted curves.
		\end{enumerate}
		This distinction can become important as the resulting curves that are compared in the derivation of the Cram\'{e}r-von Mises type test statistic might look considerably different depending on the chosen approach. In Section \ref{sim_persistence}, two heuristic simulations are performed to compare the difference in power between these approaches.

		
	\section{Simulation Study}\label{Simulation_Study}
		To learn more about the potential practical applications of this method, it is useful to study its properties in a simulation. \cite{bugni_permutation_2021} studied an array of different setups that give an idea of the performance in different settings and this thesis will replicate some of these results and explore some where the method struggles.
	
		\subsection{Implementation as an R package}
			All analyses in this thesis have been conducted with R\footcite{R}. I implemented the two-sample variant of the test presented taken from \cite{bugni_permutation_2021} in an R package called \textit{PermFDATest}. The R package and all code that has been used to produce the following results are publicly available as part of a GitHub repository\footnote{\href{https://github.com/JakobJuergens/Masters_Thesis}{https://github.com/JakobJuergens/Masters\_Thesis}} that complements this thesis.
			
			\begin{itemize}
				\item \cite{fda}
				\item \cite{tidyverse}
				\item \cite{refund}
			\end{itemize}
			
		\subsubsection{Use of High-Performance Computing}
			The simulations presented as part of this thesis have been conducted on \textit{bonna}\footnote{\href{https://www.dice.uni-bonn.de/de/hpc/hpc-a-bonn/infrastruktur}{https://www.dice.uni-bonn.de/de/hpc/hpc-a-bonn/infrastruktur}}. \textit{bonna} is the high performance computing cluster provided by the University of Bonn. The implementation is heavily parallelized and makes use of a SLURM scheduling system. However, slight modifications of the provided code suffice to run it on personal computers.

		\subsubsection{Particularities of the Implementation}
			For my implementation in the package that is provided in the GitHub repository corresponding to this thesis, I chose some slightly unusual methods to increase the package's performance. I want to present some in the following as they present opportunities for performance gains that are somewhat interesting on their own.
			\begin{itemize}
				\item For the evaluation of the empirical distribution function, I make use of a result \cite{boyd_computing_2006} that simplifies the problem of finding the zeroes of a finite Fourier series to an eigenvalue problem for a matrix that is defined in terms of the Fourier coefficients. If the chosen basis is the Fourier basis this is more rigorous than the grid based approach used for the other basis types and faster then potential numerical methods to approximate the zeroes in a more general setting.
			\end{itemize}

		\subsection{Simulation Setup}
		As part of this thesis, I take inspiration from the simulation study from \cite{bugni_permutation_2021} and shine more light on specific settings that offer insight into the potential benefits and problems of the method. To describe the specific processes used in the simulation, I introduce the notation used in \cite{bugni_permutation_2021} shortly to then describe the data generating processes mathematically. I generate the data on the closed interval $\left[0,1\right]$ by discretely generating observations and fitting a Fourier basis to the discrete data to obtain functional observations. Let $\mathcal{T} = \left\{0, 0.01, 0.02, \dots, 1\right\}$ be the index set of the discrete points on the unit interval, then we construct the observations in the following way.
		
		\begin{enumerate}
			\item Draw random variables $\left\{\xi_{i,s} \ \vert \ (i,t) \in \left\{1, \dots, 20\right\} \times \mathcal{T} ; s = 1,2 \right\}$ independently from the $\mathcal{N}(0,1)$ distribution.
			\item For all $i = 1, \dots, 20$ and $s = 1,2$, set $\tilde{X}_{i,s}(0) = \xi_{i,s}(0)$.
			\item For all $i = 1, \dots, 20$; $s = 1,2$ and $t = 0.01, \dots, 1$, set $\tilde{X}_{i,s}(t) = \rho_{s}(t)\tilde{X}_{i,s}(t-0.01) + \xi_{i,s}(t)\sqrt{1-\rho_s^2(t)}$ where $\rho_{s}(t)$ is a parameter as defined below.
			\item For all $i = 1, \dots, 20$; $s = 1,2$ and $t \in \mathcal{T}$, set $X_{i,s}(t) = \mu_s(t) + \sigma_s(t)\tilde{X}_{i,s}(t)$ where $\mu_s(t)$ and $\sigma_s(t)$ are parameters as defined below.
		\end{enumerate}
		The resulting random variables $\left\{X_{i,s}(t) \ \vert \ t \in \mathcal{T}; \ i = 1, \dots, 20; \ s = 1,2\right\}$ are normally distributed with the following properties.
		\begin{multicols}{2}
			\begin{enumerate}
				\item $\mathbb{E}\left[X_s(t)\right] = \mu_s(t)$
				\item $\text{Var}\left[X_s(t)\right] = \sigma^2_s(t)$
				\item $\text{Corr}\left[X_s(t), X_s(t-1)\right] = \rho_s(t) \\ \forall t \in \mathcal{I} \ \text{with} \ t > 1$
			\end{enumerate}
		\end{multicols}
		As can be seen in the mathematical description above, deviating from the original paper, I focus on the two sample setting and use a sample size of 20 observations per sample. I focus on three distinct violations of the Nullhypothesis described in the following. For each of the following settings I construct sample 1 identically as a reference point to compare the different settings. The parameters for sample 1 are therefore chosen as follows.
		\begin{multicols}{2}
			\begin{enumerate}
				\item $\mu_1(t) = 0 \quad \forall t = 0.01, \dots, 1$
				\item $\sigma_1(t) = 1 \quad \forall t = 0.01, \dots, 1$
				\item $ \rho_1(t) = 0.5 \quad \forall t = 0.01, \dots, 1$
			\end{enumerate}
		\end{multicols}
		For the second sample, I chose three different violations of the Null hypothesis and a benchmark case where the Null hypothesis is correct.
		\begin{enumerate}
			\item \textbf{Identical Data Generating Processes}\\
				  The first setting is the benchmark where both samples are generated using the same random function.
			\item \textbf{Mean Shift}\\
				  The second setting introduces a shifted mean function. This is one possible violation where \cite{bugni_permutation_2021} describe weak power of the Cramer-von Mises type test. Deviating from sample 1, sample 2 has mean function 
				  $$\mu_2(t) = x (x-1) \quad t \in \mathcal{T}$$
			\item \textbf{Correlation Shift}\\
			 	  The third setting violates the Null hypothesis by changing the correlation structure between neighboring observations. Here, observations are less persistent due to a lower, but still constant choice of the correlation parameter.
			 	  $$\rho_2(t) = 0.2 \quad \forall t = 0.01, \dots, 1$$
			\item \textbf{Variance Shift}\\
				  In the fourth simulation setting, the variance function is altered in the following way.
				  $$\sigma_2(t) = 1 + 0.5t \quad \quad t \in \mathcal{T}$$
		\end{enumerate}
	
		\begin{figure}[H]
			\makebox[\textwidth][c]{
				\includegraphics[width = 1.1\textwidth]{../Graphics/Settings\_comparison.PDF}
			}
			\caption{Samples generated for the four settings. \\
			Sample 1 in red, Sample 2 in blue}
			\label{settings}
		\end{figure}
		As described in the sections describing the test procedure theoretically, there is a number of parameter choices that has to be made to perform the simulation.
		\begin{enumerate}
			\item As shown in the mathematical construction of the data, I chose a smaller number of observations per sample $n=20$ and focus on the two-sample version of the test. Additionally each observation is constructed from a lower number of discrete observations (101) than in the original paper.
			\item The truncation parameter $K$ for the construction of the random function inducing the measure $\mu$ as shown in Equation \ref{truncation}. The original authors argue that their simulations showed that the power of the test became flat at about $K = 20$ and chose $K = 25$. For the sake of reproducing their results I used the same choice.
			\item The parameter $L$ determining the number of functions drawn according to $\mu$ in the approximation of the test statistic $\hat{\tau}_{n,m}$ of the Cram\'{e}r-von Mises type test. The authors of the original paper chose $L = 4000$ and preliminary testing confirmed that this choice seems to be reasonable in many settings when combined with reasonable choices for the construction of the measure $\mu$.
			\item The number of permutations $Q$ used for the approximation of the permutation distribution of both test statistics. The authors chose a value of $Q = 500$ which I copied for the sake of reproducing their results.
			\item For the construction of the measure $\mu$, the weight function $w(t)$ has to be chosen. For the purposes of this simulation, the weight function is given as the point-wise 95\% quantile of the observations of the reference sample. This is a potentially significant deviation from the approach chosen by the original authors.
			\item Deviating from the choice of the original authors due to the results of preliminary testing, I chose the sequence $\left(\rho_i\right)_{i = 1, \dots, K}$ by checking the sample standard deviation of the corresponding Fourier coefficients in the first sample. I then doubled these values to increase variability in the functions drawn from $\mu$. This higher variability seemed to improve performance in preliminary tests, but should be studied in further experiments.
			\item Using the same choice as in the original paper, I chose to use independent standard normal error terms $U_k \quad k=1, \dots, K$ for the Fourier coefficients shown in Equation \ref{Fourier_coefs} for the construction of the measure. As the processes under consideration are Gaussian by construction, this choice is reasonable. For distributions with heavier tails, the authors recommend using different error terms and it would be interesting to study the effects of different choices.
		\end{enumerate}
		
		\subsection{Results}
		Taking a look at the results from the simulation study, it is interesting to compare the empirical rejection probabilities for different types of violations of the Nullhypothesis.
		
		\begin{table}[h!]
			\centering
			\begin{tabular}{lccccc}\toprule
				\textbf{Test}	&$\left(\alpha_{\tau}, \alpha_{\nu}\right) $ &\textbf{Setting 1} &\textbf{Setting 2}	&\textbf{Setting 3} &\textbf{Setting 4}\\
				\midrule
				Means-Test		&$\alpha_{\nu} = 0.05$	& 0.058			& 0.519  	 & 0.057	  & 0.057 		\\
				CvM-Test 		&$\alpha_{\tau} = 0.05$	& 0.04147313	& 0.1842943  & 0.09938171 & 0.1350278	\\
				\midrule
				Combined		& (0.04, 0.01)			& 0.04353598	& 0.3652623  & 0.09063807 & 0.1272009 	\\
								& (0.03, 0.02)			& 0.0527092		& 0.4280618  & 0.08540022 & 0.121842 	\\
								& (0.025, 0.025)		& 0.05417252	& 0.4456977  & 0.08476733 & 0.1137233 	\\
								& (0.02, 0.03)			& 0.05297339	& 0.4585891  & 0.07644395 & 0.1093622 	\\
								& (0.01, 0.04)			& 0.05927385	& 0.4860989  & 0.06362453 & 0.1001554 	\\
				\bottomrule
			\end{tabular}
			\caption{Empirical Rejection Probabilities}
		\end{table}
		Even though these rejection frequencies seem low at first, one has to keep in mind the magnitude of the violation of the Null hypothesis and the small sample size. Looking at Figure \ref{settings}, differences between the processes are not particularly stark visually and a low power therefore somewhat expected. Additionally, the power of permutation tests is highly dependent on the sample size of the underlying samples. Therefore, a rather small sample size of 20 as used in these simulations justifies the rejection frequencies.  It would be interesting to repeat these simulations for a number of different sample sizes as this could give a more precise idea of the context in which this procedure has higher power.
	
		\subsection{Simulation - Test for Persistence Alternatives}\label{sim_persistence}
		Using simulation setting 3, it is possible to also test the proposed test for persistence alternatives from Section \ref{closed_persistence_test}. But due to computational constraints, this simulation does not follow the real test structure and shall instead serve as a heuristic for the real test's performance. Mainly the simulation differs from the proposed test in the following way to reduce computational cost while staying reasonably accurate in the proposed simulation setting. The standardization step makes it necessary to perform the comparison between functions that is performed in the calculation of the Cram\'{e}r-von Mises test statistic in each permutation step. This massively increases the computational cost. Therefore, as a heuristic for the real test, these simulations are performed without a standardization step. However, a data generating process with a mean function $\mu(t) = 0$ and a point-wise variance function $\sigma(t) = 1$ is used.
		Therefore, in expectation sample equivalents will share these characteristics. However, individual samples may not. Due to this concession to limited computational resources the following simulation results could potentially overstate the power of the test against alternatives as described above. This is caused by potential differences in the sample mean and point-wise sample variance present in the permuted samples that would have been removed by standardization. 
		However, in the proposed test structure the possibility to specifically test for alternatives in the persistence structure is a significant advantage.\\
	
		The results for different correlation parameters are given in the following tables. Again, the same reference process with $\rho_1(t) = 0.5$ generated the reference sample.
		\begin{table}[h!]
			\centering
			\begin{tabular}{lccccc}\toprule
				$\alpha_{\tau}$ &$\rho_2(t) = -0.9$ &$\rho_2(t) = -0.5$ &$\rho_2(t) = 0$ &$\rho_2(t) = 0.5$ &$\rho_2(t) = 0.9$\\
				\midrule
				$0.05$	& 0.830 & -  &  0.250	& 0.0415   & - \\
				$0.01$ 	& 0.783	& -  &  0.119	& 0.0104   & -  \\
				$0.001$	& 0.735	& -  &  0.0492	& 0.00261  & -  \\
				\bottomrule
			\end{tabular}
			\caption{Empirical Rejection Probabilities}
		\end{table}
	                                          	
		The apparent power of this test comes in part from the basis that is used to fit the data. Even though standardization of the original observations as described in approach one shown in Section \ref{persistence_test} ensures that the discrete data itself has a point-wise sample variance of one, the method that is used to fit the observations can lead to curves that are highly dissimilar between samples once the data points are fitted. 
		Figure \ref{curve_fitting_artefact} shows a rather extreme case for the case of $\rho_1(t) = 0.5$ and $\rho_2(t) = -0.9$. In Appendix \ref{add_figures} Figure \ref{persistence_samples} shows some original data sets generated for these simulations.These are contrasted by Figure \ref{persistence_samples_basis_problem} which shows the same samples fitted using a Fourier basis containing 25 functions. 
		Even though the underlying data generating processes have an identical point-wise variance, the method that is used to fit the discrete observations, creates potentially distorting artifacts.
		\begin{figure}[H]
			\makebox[\textwidth][c]{
			\includegraphics[width=1.1\textwidth]{../Graphics/rho0_9_comparison.PDF}
			}
		\caption{Artifact of the Curve Fitting Procedure}
		\label{curve_fitting_artefact}
		\end{figure}
		In some settings this could be a desirable property of the test. However, it is difficult to predict how this property could influence the test's performance in real world settings. Therefore, it could be advisable to modify the test in one of two ways to mitigate these artifacts.
		\begin{enumerate}
			\item Standardize the samples of fitted curves to have a point-wise mean of zero and a point-wise variance of one.
			\item Using methods to compare the functions that do not rely on fitting a functional basis to the original observations.
		\end{enumerate}
		The first option 
		The latter option could for example be implemented in the following ways.
		\begin{enumerate}
			\item Compare only the actual discrete observations with their corresponding values on the functions generated for Monte-Carlo integration.
			\item Use linear interpolation on the observations to compare points on a chosen grid with their corresponding values.
		\end{enumerate} 
		Variant one can be seen as close to approach two from Section \ref{persistence_test} as the standardization is applied to every point that is actually considered during the comparison of the functions for the Cra\'{e}r-von Mises test. The point-wise functional standardization mentioned in \ref{persistence_test} is computationally intensive and requires a more advanced method of comparison between functions. 
		However, as variant one described above can be seen as a heuristic for its performance, I performed a second simulation using this approach, leading to the results presented in Table \ref{rej_probs_cor}. These could potentially be a better approximation of the power of the test procedure as the rejections do not rely on artifacts created by the process of fitting a functional basis to the discrete observations.
		\begin{table}[h!]
			\centering
			\begin{tabular}{lccccc}\toprule
				$\alpha_{\tau}$ &$\rho_2(t) = -0.9$ &$\rho_2(t) = -0.5$ &$\rho_2(t) = 0$ &$\rho_2(t) = 0.5$ &$\rho_2(t) = 0.9$\\
				\midrule
				$0.05$	& - & -  &  - & -   &  \\
				$0.01$ 	& -	& -  &  - & -   &  \\
				$0.001$	& -	& -  &  - & -  &  \\
				\bottomrule
			\end{tabular}
			\caption{Empirical Rejection Probabilities}
			\label{rej_probs_cor}
		\end{table}

				
	\section{Application}\label{Application}
		To test the real-world merits of the method, I will compare electricity demand data from Adelaide which is provided as part of the \textit{fds}\footcite{fds} package for R. This data set presents half-hourly energy demand in megawatts and was originally used by \cite{magnano_generation_2007} and \cite{magnano_generation_2008}. It contains electricity demand curves for 3556 days from 6/7/1997 to 31/3/2007. Some of these curves, specifically a selection of curves observed on Wednesdays and Saturdays, is shown in Figure \ref{electricity_demand}.
		\begin{figure}[H]
			\makebox[\textwidth][c]{
				\includegraphics[width=1.1\textwidth]{../Graphics/electricity_demand_curves.PDF}
			}
			\caption{Electricity Demand in Adelaide}
			\label{electricity_demand}
		\end{figure}
	
		The question I want to study with the method presented in this thesis is whether electricity demand on working days and weekends can be seen as if they were generated by the same stochastic process. However, due to information obtained during the data cleaning step, that indicates a violation of the i.i.d. assumption, the observations for the weekend will be limited to Saturdays. \\
		As this problem does not have the structure that an experiment as described in \cite{bugni_permutation_2021} possesses, a few problems have to be addressed before using the procedure. These adjustments are described below and some details are substantiated in Appendix \ref{Application_Appendix}.
	
		\subsection{Data Cleaning and Preprocessing}
			\subsubsection{Removal of Mondays and Fridays}
			One potential problem of this procedure is the question whether observations on different weekdays or days of the weekend can be seen as independent and identically distributed. While it seems reasonable to assume that demand may be similar on Saturdays and Sundays, it is questionable whether the same can be said for working days. 
			One potential problem is the ramping up and down of industrial production and commercial activity on Mondays and Fridays. Therefore, I decided to exclude these days from my analysis and only compare the weekend with Tuesdays, Wednesdays and Thursdays.
			
			\subsubsection{Holidays}
			Furthermore, holidays could appear more regularly on specific weekdays than others. Whereas on weekends, a holiday would not significantly influence the electricity demand due to the already reduced economic activity, this is different for weekdays. Therefore if holidays would occur systematically more often on specific days - such as for example Thursdays for the case of Germany - this could create problems. Therefore public holidays in Southern Australia, the Australian federal territory containing Adelaide, were excluded in the analysis. A list of holidays that were excluded is given in Appendix \ref{Application_Appendix}. Additionally, days immediately before and after holidays are excluded due to the same reason as Mondays and Fridays. This procedure of eliminating holidays from the data set removed 299 out of 3556 curves from the data set which was used in further steps of the analysis. \\
			
			\subsubsection{Detrending and Deseasoning}
			A third potential problem of this data set is its functional time series structure. For example, electricity demand might be systematically higher in the summer months due to the added energy consumption of air-conditioning units. Therefore, a simple interpretation of the data as generated by an i.i.d. process might be unsubstantiated and additional steps have to be made before the procedure can be justified. \\
			Additionally, it might be the case that electricity demand has a trend component that has to be removed before this method can reasonably be applied to this data. To combat this low frequency seasonal component due to the seasons and a potential long-term trend I specify a model as follows and demean the data as described below. For the estimation of this model, holidays and days immediately before and after holidays are excluded. Mondays and Fridays are included and removed from the data set after the estimation for the creation of the samples that are used to apply the method described in this thesis.
			\begin{equation}\label{Data_Cleaning}
				\begin{split}
					f_{\textit{demand}} = &f_{\textit{mean}} + f_{\textit{trend}}(\textit{year} - 1997) + \sum_{j = 2}^{12}\mathbbm{1}_{\left[\textit{month}\: = \: j\right]}f_{\textit{month}, j}\\
					 &+ \sum_{k = 2}^{7}\mathbbm{1}_{\left[\textit{day}\: = \: k\right]}f_{\textit{day}, k} + f_{\textit{random}}
				\end{split}	
			\end{equation}
			This is estimated with the usual theory for function-on-scalar regression which is described for example in \cite{ramsay_functional_2005}. Then, the following objects are used for the further treatment.
			\begin{equation}
				\tilde{f} = f_{\textit{mean}} + \sum_{k = 2}^{7}\mathbbm{1}_{\left[\textit{day}\: = \: k\right]}\hat{f}_{\textit{day}, k} + \hat{f}_{\textit{random}}
			\end{equation}
			The function-on-scalar regression described in Equation \ref{Data_Cleaning} was performed in R using the \textit{fda}\footcite{fda} package and gave the following results. As these are the results of a function-on-scalar regression, it is convenient to plot the resulting estimates as they are functions instead of giving the estimated Fourier coefficients.
			
			\begin{figure}[H]
				\makebox[\textwidth][c]{
				\includegraphics[width=1.1\textwidth]{../Graphics/estimate_const_year.PDF}
			}
				\caption{Estimates for the constant and year}
				\label{estimates_const_year}
			\end{figure}
		
			\begin{figure}[H]
				\makebox[\textwidth][c]{
				\includegraphics[width=1.1\textwidth]{../Graphics/estimate_months.PDF}
			}
				\caption{Estimates for the months (January as baseline)}
				\label{estimates_months}
			\end{figure}
		
			The estimated coefficient functions for the different weekdays are of special interest as they are directly linked to the problem that is to be studied using the method from \cite{bugni_permutation_2021}. In this case Sunday is the baseline and these curves describe the deviation relative to it.
			\begin{figure}[H]
				\makebox[\textwidth][c]{
				\includegraphics[width=1.1\textwidth]{../Graphics/estimate_weekdays.PDF}
			}
				\caption{Estimates for the weekdays (Sunday as baseline)}
				\label{estimates_weekdays}
			\end{figure}
			This diagram already hints at a considerable mean shift between the working days and the weekend. As these estimates also hint at a considerable difference between Saturdays and Sundays, it could be advisable to treat them as non-identically distributed. Therefore,the application is limited to a comparison of working days and Saturdays as the results of the regression provide clear evidence against the assumption that Saturday and Sunday can be seen as if they were generated by the same random variable.
			
			
		\subsection{Test from \cite{bugni_permutation_2021}}
			After cleaning the data as described in the previous section, we can now apply the method from \cite{bugni_permutation_2021}.
			To give a visualization of what these cleaned curves look like, Figure \ref{electricity_demand_cleaned} shows randomly chosen curves observed on Wednesdays and Saturdays that were prepared with the procedure described above.
			
			\begin{figure}[H]
				\makebox[\textwidth][c]{
				\includegraphics[width=1.1\textwidth]{../Graphics/electricity_demand_curves_cleaned.PDF}
			}
				\caption{Pre-Processed Electricity Demand in Adelaide}
				\label{electricity_demand_cleaned}
			\end{figure}
			
			Applying this test to the cleaned data leads to the following p-values for the two underlying tests.
			\begin{multicols}{2}
				\begin{itemize}
					\item $p_\tau = $ {\color{red}hier p-value ergaenzen}
					\item $p_\nu = $ {\color{red}hier p-value ergaenzen}
				\end{itemize}
			\end{multicols}
			Therefore using the same decision rules as in the simulation study, we come to the following rejection decisions.
			\begin{table}[h!]
				\centering
				\begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} c @{\extracolsep{\fill}} c}
					\toprule
					\textbf{Test}	&$\left(\alpha_{\tau}, \alpha_{\nu}\right) $ &\textbf{Rejection of the Null?} \\
					\midrule
					Means-Test		& $\alpha_{\nu} = 0.05$		& \checkmark \\
					CvM-Test 		& $\alpha_{\tau} = 0.05$	& \checkmark \\
					\midrule
					Combined		& (0.04, 0.01)				& \checkmark	\\
									& (0.03, 0.02)				& \checkmark	\\
									& (0.025, 0.025)			& \checkmark	 \\
									& (0.02, 0.03)				& \checkmark	\\
									& (0.01, 0.04)				& \checkmark	\\
					\bottomrule
				\end{tabular*}
				\caption{Results of the Test from \cite{bugni_permutation_2021}}
			\end{table}
			These rejections of the Null hypothesis are unsurprising when looking at the curves that were generated for the different days in the data cleaning step. There, it was already obvious that the difference in mean alone was stark between workdays and Saturdays. Therefore, it was expected for the means based test to find the violation of the Null hypothesis in this comparatively large sample problem. Due to the large differences in mean, it is also unsurprising that the Cram\'{e}r-von Mises type test picked up on the violation.\\
			
			This application can therefore serve more as a real-world proof of concept for the test. In reality most scenarios where applying this test will be interesting will be less obvious in the violation of the Null hypothesis. Especially violations created by differing persistence structures will typically be difficult to identify. Therefore, it would be interesting to apply this test to some more challenging settings. A small selection of settings is therefore presented in Section \ref{Outlook}.
	
	\section{Outlook}\label{Outlook}
		After studying the method developed by \cite{bugni_permutation_2021} in detail there are a few points that might be interesting for further research. In the following I name a few that could be very interesting extension to this thesis.
		
		\subsection{Possible Further Simulations}
			\paragraph{Comparing choices of $w(t)$ and  $\left(\rho_i\right)_{i = 1, \dots, K}$\\}
			In this thesis I only perform simulations for one intuitive choice of $w(t)$ and $\left(\rho_i\right)_{i = 1, \dots, K}$. It seems reasonable to assume that these parameter choices could have a significant impact on the performance of the method and it would therefore be interesting to compare different choices in a structured framework.
			
			\paragraph{Less restrictive Distributions of Fourier Coefficients\\}
			The measures constructed as part of the Cram\'{e}r-von Mises type test described in this thesis drew their Fourier coefficients using independently distributed error terms. Even though this leads to a convenient implementation, using measures induced by random variables generated using error terms with a more general dependence structure might be beneficial to increase the power against specific alternatives. It would therefore be interesting to study the influence on the measure on the power of the test in more general settings both from a theoretical perspective and using simulations. 
		
			\paragraph{Simulations for Unbalanced Sample Sizes\\}
			The simulations in this thesis only dealt with settings in which both the reference sample and the second sample contained the same number of curves. Even though there is no obvious theoretical reason for unbalanced sample sizes to influence the power of the test meaningfully, it would be interesting to study settings with unbalanced sample sizes in further simulations.		
			
			\paragraph{Comparing Implementations using different orders of Operations\\}
			As described in Sections \ref{persistence_test} and \ref{sim_persistence}, the actual implementation could be highly important for the properties of the proposed test for persistence alternatives. This thesis only provided two heuristic simulations to compare the performance of the proposed options due to computational limitations. Therefore, it would be interesting to actually compare these options in further simulations.
		
		\subsection{Potential Applications}
			As explained in Section \ref{Application}, the chosen setting is suitable to apply the test, but it is more of a showcase then what this method would be used for in reality. Therefore, it would be interesting to apply the method and the variant for persistence alternatives to data that is more fitting. Two examples that could provide interesting applications for this method are listed in the following.
			\begin{itemize}
				\item \textbf{Audio-Curves} and specifically speech recordings are a typical scenario for functional data. The tests described in this thesis could be used to compare samples of recordings of spoken words. One interesting application could be to determine whether a set of recordings was manipulated when there is an appropriate reference sample that is known to be authentic. Especially in the context of computer generated speech imitation, sometimes known as voice deep-fakes, a test that could reliably distinguish between original and imitated speech recordings could be highly relevant.
				\item \textbf{Movement-Curves} such as for example curves describing the movement of a runner's legs during a 100m sprint might also be an interesting field for potential applications. Given a reference sample of movement curves of runners with a specific alteration to their movement due to e.g. a sickness, the test might be useful in the context of medical diagnostics.
			\end{itemize}
		
		
	
	\newpage
	\section{Bibliography}
	\printbibliography[heading=none]
	
	\newpage
	\cleardoublepage
	\pagenumbering{roman}
	\setcounter{page}{1}
	\section{Appendix}
	
		\subsection{Informal Intuition}\label{Intuition}
			For potential readers that are unfamiliar with the idea of functional data analysis and permutation testing, the initial hurdle of reading this thesis without any intuitive knowledge about the ideas at play might be very high. Therefore, this short informal introduction should shall serve as a primer that gives an intuitive idea about the following questions.
			\begin{enumerate}
				\item What setting are we in?
				\item What do we want to test?
				\item How do we want to test it?
				\item Why should I care?
			\end{enumerate}
			By choice, this section will \textbf{not} be formal and it will \textbf{not} be precise as to not introduce too much detail that could hinder an intuitive understanding.\\
			
			Answering question one is relatively simple. This thesis deals with a statistical problem, where observations are continuously observed curves. But what does that mean intuitively? One scenario that is easy to imagine, is a curve over time for some variable such as speed: When driving a car at the 24 hour race at Le-Mans, it has a speed at every point in time during the race - for example $155.3$ km/h at 2 hours, 33 minutes and 7 seconds after the race has started. Observations in the sense of this thesis are similar to this curve: at every value of $x$ in some closed interval $[a,b] \subset \mathbb{R}$, we observe a value $y$.\\
			
			Question two is also simple. What we want to test is if two samples of curves are similar in a specific way. As in many statistical problems, we interpret the observations as realizations of some random variable. The only difference being that these observations are curves. So we can ask the question whether the curves in the two samples are dissimilar enough for us to say confidently that they were not generated by the same random variable. 
			Staying with the example of Le-Mans, we could have two identical cars with two equally skillful drivers collect 24 hour speed curves for the next 10 years. We do this to find out if two different types of fuel change the way the cars act on the track. In this hypothetical scenario, we now have two samples each containing 3650 speed curves. 
			We want to test statistically whether changing the fuel made any difference.\\
			
			Question three is more difficult. Therefore, I want to give the right intuition by answering simpler question instead. Let's return to the scalar setting for a moment - each observation is just a real number $x \in \mathbb{R}$ again. We want to find out whether two samples are different enough for us to say confidently: ``These samples are too dissimilar to be generated by the same random variable.'' If you have some statistical knowledge, your first intuition might be to perform a Kolmogoroff-Smirnov, Cram\'{e}r-von Mises or Anderson-Durbin test. But let's take one step back first and think about a more general idea. \\
			From an intuitive point of view, if the two samples were generated by the same random variable, the effect of randomly switching observations between the samples should be small. Formalizing this idea leads to the concept of permutation tests. By randomly permuting the samples and calculating a chosen test statistic on the permuted samples, one could derive a distribution for said test statistic that can be used for testing. If the test statistic calculated on the non-permuted samples is comparatively extreme, this could be an indicator that the samples were not generated by the same random variable.\\
			
			Why should you care about this is the hardest question. I care about it, because it is cool from a mathematical perspective. But if you care about real-life problems there are also good reasons to be interested. Economic data is being observed at increasing frequencies as technology improves. Many problems that were previously low dimensional due to data constraints or suitable for classical time series methods, are becoming more complex as methodological challenges such as high-dimensionality and extreme correlation of neighboring observations arise. Functional data analysis is a comparatively new approach to many of these problems that transforms some of these problems into strengths by acknowledging the functional structure of the underlying processes. Permutation tests are also comparatively new, at least in our ability to apply them on a larger scale.
			
	
		\subsection{Multiple Testing}\label{Multiple_Testing}
			When testing statistical hypotheses, it is often helpful or even necessary to test multiple hypotheses independently of each other. One setting where this could be useful is when we want to combine the desirable properties of two tests, as is done by \cite{bugni_permutation_2021}. If the tests do not perfectly depend on each other, this creates a problem relating to the size of the combined test.
			
			\begin{definition}[Family-wise Error Rate]
				The family-wise error rate is the probability of making at least one type-1 error when performing multiple hypothesis tests.
			\end{definition}
		
				The most straightforward correction for this multiple testing problem is the so-called Bonferroni Correction. Introduced by \cite{dunn_multiple_1961}, it is based on Boole's Inequality, which is sometimes referred to as the Bonferroni Inequality.
				\begin{equation}
						\mathbb{P}\left[\bigcup_{i = 1}^{\infty} A_i\right] \leq \sum_{i = 1}^{\infty} \mathbb{P}\left[A_i\right]
					\end{equation}
				for a countable set of events $A_1, A_2, \dots$.
		
		\subsection{Derivation of Asymptotic Distribution}\label{asymp_deriv}
			Here I will derive the asymptotic distribution
			
		\subsection{Application Data Cleaning}\label{Application_Appendix}
			\subsubsection{Excluded Holidays}
			Holidays that were excluded in the analysis are the following\footnote{These were taken from \url{https://www.australia.gov.au/public-holidays} accessed on 20.05.2022.}: New Year's Day, Australia Day, March Public Holiday, Good Friday, Holy Saturday, Easter Monday, Anzac Day, Queen's Birthday, Labour Day, Christmas Day, Christmas Eve, Christmas Day, Proclamation Day and New Year's Eve. Sunday is nominally a public holiday in South Australia. Easter Sunday is therefore not a special public holiday, but due to its prominence it was excluded in this analysis.
			
			As Australia replaces some holidays that fall on weekends with substitute holidays on the next working day, these were also excluded. For the South Australia, this can occur for New Year's Day, Australia Day, ANZAC Day, Christmas Day, Proclamation Day and New Year's Eve.
		
			\subsubsection{Detrending and Deseasoning Results}
			
		\subsection{Additional Figures}\label{add_figures}
			\begin{figure}[H]
				\makebox[\textwidth][c]{
					\includegraphics[width=1.1\textwidth]{../Graphics/persistence_comparison.PDF}
				}
				\caption{Original Samples for Persistence Test}
				\label{persistence_samples}
			\end{figure}
			
		
			\begin{figure}[H]
				\makebox[\textwidth][c]{
					\includegraphics[width=1.1\textwidth]{../Graphics/persistence_comparison_basis.PDF}
				}
				\caption{Samples for Persistence Test fitted using a Fourier Basis with 25 functions}
				\label{persistence_samples_basis_problem}
			\end{figure}
		
		
		
	\newpage
	\thispagestyle{empty}
	\section*{Versicherung an Eides statt}	
	
		\vspace{3cm}
		
		Ich versichere hiermit, dass ich die vorstehende Masterarbeit
		selbstst√§ndig verfasst und keine anderen als die angegebenen Quellen
		und Hilfsmittel benutzt habe, dass die vorgelegte Arbeit noch an keiner
		anderen Hochschule zur Pr√ºfung vorgelegt wurde und dass sie weder
		ganz noch in Teilen bereits ver√∂ffentlicht wurde. W√∂rtliche Zitate und
		Stellen, die anderen Werken dem Sinn nach entnommen sind, habe ich
		in jedem einzelnen Fall kenntlich gemacht.
		
		\vspace{2cm}
		Bonn, XX.07.2021 \hrulefill \\
		\hspace*{0mm}Jakob R. Juergens
		
		\vspace{\fill}
\end{document}